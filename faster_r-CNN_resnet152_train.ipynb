{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN\n",
    "from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import shutil \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_ratio = 0.1\n",
    "batch_size=8\n",
    "seed = 0\n",
    "# train_device = Device.GPU\n",
    "number_of_epochs = 3\n",
    "learning_rate = 0.0001\n",
    "weight_decay = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "root_data_dir = Path(\"kaggle/input/global-wheat-detection/\")\n",
    "@dataclass\n",
    "class DatasetArguments:\n",
    "    data_dir: Path\n",
    "    images_lists_dict: dict\n",
    "    labels_csv_file_name: str\n",
    "\n",
    "@dataclass\n",
    "class DataLoaderArguments:\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    dataset_arguments: DatasetArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_images_file_names_from_csv(directory):\n",
    "    dataframe = pd.read_csv(os.path.join(directory, \"train.csv\"))\n",
    "    files = dataframe[\"image_id\"].unique().tolist()\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _choose_train_valid_file_names(file_names, valid_numbers, seed):\n",
    "    np.random.seed(seed)\n",
    "    valid_file_names = np.random.choice(file_names, valid_numbers, replace=False).tolist()\n",
    "    train_file_names = [file_name_i for file_name_i in file_names if file_name_i not in valid_file_names]\n",
    "    return train_file_names, valid_file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = _get_images_file_names_from_csv(root_data_dir)\n",
    "valid_numbers = round(len(file_names) * test_train_ratio)\n",
    "train_file_names, valid_file_names = _choose_train_valid_file_names(file_names, valid_numbers, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_lists_dict = {\n",
    "    \"train\": train_file_names,\n",
    "    \"val\": valid_file_names,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arguments = DatasetArguments(\n",
    "    data_dir=root_data_dir,\n",
    "    images_lists_dict=images_lists_dict,\n",
    "    labels_csv_file_name=\"train.csv\",\n",
    ")\n",
    "\n",
    "dataloaders_arguments = DataLoaderArguments(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    "    dataset_arguments=dataset_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_set():\n",
    "    transforms_dict = {\n",
    "        'train': get_train_transforms(),\n",
    "        'val': get_valid_transforms()\n",
    "    }\n",
    "    return transforms_dict\n",
    "\n",
    "\n",
    "def get_train_transforms():\n",
    "    return Compose(\n",
    "        [OneOf([HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2,\n",
    "                                   val_shift_limit=0.2, p=0.9),\n",
    "                RandomBrightnessContrast(brightness_limit=0.2,\n",
    "                                         contrast_limit=0.2, p=0.9)],\n",
    "               p=0.9),\n",
    "            ToGray(p=0.01),\n",
    "            HorizontalFlip(p=0.5),\n",
    "            VerticalFlip(p=0.5),\n",
    "            Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0)],\n",
    "        p=1.0,\n",
    "        bbox_params=BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return Compose(\n",
    "        [\n",
    "            ToTensorV2(p=1.0),\n",
    "        ],\n",
    "        p=1.0,\n",
    "        bbox_params=BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0,\n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _adjust_boxes_format(boxes):\n",
    "    # original format [xmin, ymin, width, height]\n",
    "    # new format [xmin, ymin, xmax, ymax]\n",
    "    adjusted_boxes = []\n",
    "    for box_i in boxes:\n",
    "        adjusted_box_i = [0, 0, 0, 0]\n",
    "        adjusted_box_i[0] = box_i[0]\n",
    "        adjusted_box_i[1] = box_i[1]\n",
    "        adjusted_box_i[2] = box_i[0] + box_i[2]\n",
    "        adjusted_box_i[3] = box_i[1] + box_i[3]\n",
    "        adjusted_boxes.append(adjusted_box_i)\n",
    "    return adjusted_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _areas(boxes):\n",
    "    # original format [xmin, ymin, width, height]\n",
    "    areas = []\n",
    "    for box_i in boxes:\n",
    "        areas.append(box_i[2] * box_i[3])\n",
    "    return areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, images_root_directory,\n",
    "                 images_list,\n",
    "                 labels_csv_file_name,\n",
    "                 phase,\n",
    "                 transforms):\n",
    "        super(ObjectDetectionDataset).__init__()\n",
    "        self.images_root_directory = images_root_directory\n",
    "        self.phase = phase\n",
    "        self.transforms = transforms\n",
    "        self.images_list = images_list\n",
    "        if self.phase in [\"train\", \"val\"]:\n",
    "            self.labels_dataframe = pd.read_csv(os.path.join(images_root_directory, labels_csv_file_name))\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        sample = {\n",
    "            \"local_image_id\": None,\n",
    "            \"image_id\": None,\n",
    "            \"labels\": None,\n",
    "            \"boxes\": None,\n",
    "            \"area\": None,\n",
    "            \"iscrowd\": None\n",
    "        }\n",
    "\n",
    "        image_id = self.images_list[item]\n",
    "        image_path = os.path.join(self.images_root_directory,\n",
    "                                  \"train\" if self.phase in [\"train\", \"val\"] else \"test\",\n",
    "                                  image_id + \".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "        image /= 255.0\n",
    "        sample[\"local_image_id\"] = image_id\n",
    "        sample[\"image_id\"] = torch.tensor([item])\n",
    "        if self.phase in [\"train\", \"val\"]:\n",
    "            boxes = self.labels_dataframe[self.labels_dataframe.image_id == image_id].bbox.values.tolist()\n",
    "            boxes = [eval(box_i) for box_i in boxes]\n",
    "            areas = _areas(boxes)\n",
    "            boxes = _adjust_boxes_format(boxes)\n",
    "\n",
    "            sample[\"labels\"] = torch.ones((len(boxes),), dtype=torch.int64)\n",
    "            sample[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            sample[\"area\"] = torch.as_tensor(areas, dtype=torch.float32)\n",
    "            sample[\"iscrowd\"] = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "        if self.transforms is not None:\n",
    "            sample[\"image\"] = image\n",
    "            transformed_sample = self.transforms(image=sample[\"image\"],\n",
    "                                                 bboxes=sample[\"boxes\"],\n",
    "                                                 labels=sample[\"labels\"])\n",
    "            image = transformed_sample[\"image\"]\n",
    "            sample[\"boxes\"] = torch.as_tensor(transformed_sample[\"bboxes\"], dtype=torch.float32)\n",
    "            del sample[\"image\"]\n",
    "        return image, sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(arguments):\n",
    "    dataset = ObjectDetectionDataset(arguments.data_dir,\n",
    "                                     arguments.images_lists_dict[arguments.phase],\n",
    "                                     arguments.labels_csv_file_name,\n",
    "                                     arguments.phase,\n",
    "                                     arguments.transforms)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datasets_dictionary(arguments, input_size):\n",
    "    data_transforms = transform_set()\n",
    "    image_datasets = {\n",
    "        'train': None,\n",
    "        'val': None\n",
    "    }\n",
    "    for phase in ['train', 'val']:\n",
    "        arguments.phase = phase\n",
    "        arguments.transforms = data_transforms[phase]\n",
    "        image_datasets[phase] = create_dataset(arguments)\n",
    "    return image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders_dictionary(arguments, input_size):\n",
    "    batch_size = arguments.batch_size\n",
    "    num_workers = arguments.num_workers\n",
    "    image_datasets = create_datasets_dictionary(arguments.dataset_arguments, input_size)\n",
    "    dataloaders_dict = {x: DataLoader(image_datasets[x],\n",
    "                                      batch_size=batch_size,\n",
    "                                      shuffle=True,\n",
    "                                      pin_memory=True,\n",
    "                                      num_workers=num_workers,\n",
    "                                      collate_fn=collate_fn) for x in ['train', 'val']}\n",
    "    return dataloaders_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasterrcnn_resnet101_fpn(pretrained=False, progress=True,\n",
    "                            num_classes=91, pretrained_backbone=True,\n",
    "                             trainable_backbone_layers=3, **kwargs):\n",
    "    assert trainable_backbone_layers <= 5 and trainable_backbone_layers >= 0\n",
    "    # dont freeze any layers if pretrained model or backbone is not used\n",
    "    if not (pretrained or pretrained_backbone):\n",
    "        trainable_backbone_layers = 5\n",
    "    if pretrained:\n",
    "        # no need to download the backbone if pretrained is set\n",
    "        pretrained_backbone = False\n",
    "    backbone = resnet_fpn_backbone('resnet152', pretrained_backbone)\n",
    "    model = FasterRCNN(backbone, num_classes, **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = fasterrcnn_resnet101_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_device(train_device):\n",
    "    if train_device == 'GPU':\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda:0\")\n",
    "        else:\n",
    "            raise ValueError(\"No GPU was found\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_training_device('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetArguments:\n",
    "    data_dir: Path\n",
    "    images_lists_dict: dict\n",
    "    labels_csv_file_name: str\n",
    "\n",
    "@dataclass\n",
    "class DataLoaderArguments:\n",
    "    batch_size: int\n",
    "    num_workers: int\n",
    "    dataset_arguments: DatasetArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_arguments = DatasetArguments(\n",
    "    data_dir=root_data_dir,\n",
    "    images_lists_dict=images_lists_dict,\n",
    "    labels_csv_file_name=\"train.csv\",\n",
    ")\n",
    "\n",
    "dataloaders_arguments = DataLoaderArguments(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    "    dataset_arguments=dataset_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = create_dataloaders_dictionary(dataloaders_arguments,input_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = len(dataloaders[\"train\"].dataset)\n",
    "number_of_iteration_per_epoch = int(train_dataset_size / dataloaders_arguments.batch_size)\n",
    "total_number_of_iteration = number_of_epochs * number_of_iteration_per_epoch\n",
    "learning_rate_step_size = 2 * number_of_iteration_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learnable_parameters(model, feature_extract):\n",
    "    params_to_update = model.parameters()\n",
    "\n",
    "    if feature_extract:\n",
    "        params_to_update = []\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\", name)\n",
    "    return params_to_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_to_update = get_learnable_parameters(model, feature_extract=False)\n",
    "optimizer = optim.Adam(params_to_update, lr=learning_rate, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,\n",
    "                                                              T_0=learning_rate_step_size,\n",
    "                                                              T_mult=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_model(model, model_path):\n",
    "    torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_checkpoint(epoch, model, optimizer, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch,model, optimizer):\n",
    "    model_path = f\"best_model_152_epoch_{epoch}.pth\"\n",
    "    _save_model(model.state_dict(), model_path)\n",
    "    checkpoint_path = f\"checkpoint_{epoch}.pth\"\n",
    "    _save_checkpoint(epoch, model, optimizer, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    def fit_model(self):\n",
    "        start_epoch = 0\n",
    "        iteration_i = 0\n",
    "        for current_epoch in range(start_epoch, number_of_epochs):\n",
    "            running_loss = 0\n",
    "            print(f\"Starting Epoch: {current_epoch}\")\n",
    "            progress_bar = tqdm(dataloaders[\"train\"])\n",
    "            for inputs, labels in  progress_bar:\n",
    "                running_loss_i = self.training_round(inputs, labels)\n",
    "                running_loss += running_loss_i\n",
    "                current_running_error = running_loss/((iteration_i - \n",
    "                                                      current_epoch * \n",
    "                                                      number_of_iteration_per_epoch + 1)*batch_size)\n",
    "                progress_bar.set_description(f\"Running train loss: {current_running_error}\")\n",
    "                iteration_i += 1\n",
    "            epoch_loss = running_loss / len(dataloaders[\"train\"].dataset)\n",
    "            print(f\"Finishing Current epoch: {current_epoch} ... training loss: {epoch_loss}\")\n",
    "            print(\"saving the model and checkpoint: \")\n",
    "            save_model(current_epoch, model, optimizer)\n",
    "            for inputs, labels in tqdm(dataloaders[\"val\"]):\n",
    "                self.validation_round(inputs, labels)\n",
    "\n",
    "    def training_round(self, inputs, labels):\n",
    "        inputs = list(image.to(device) for image in inputs)\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = [{k: v.to(device) for k, v in t.items() if not isinstance(v, str)} for t in labels]\n",
    "        model.train()\n",
    "        loss_dict = model(inputs, labels)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        lr_scheduler.step()\n",
    "        running_loss_i = loss.item() * inputs.size(0) \n",
    "        return running_loss_i\n",
    "\n",
    "    def validation_round(self, inputs, labels):\n",
    "        model.eval()\n",
    "        inputs = list(image.to(device) for image in inputs)\n",
    "        inputs = torch.stack(inputs)\n",
    "        labels = [{k: v.to(device) for k, v in t.items() if not isinstance(v, str)} for t in labels]\n",
    "        outputs = model(inputs)\n",
    "        outputs = [{k: v.to(\"cpu\") for k, v in t.items()} for t in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector  =  Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/380 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running train loss: 1.0260122391738389: 100%|██████████| 380/380 [12:32<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing Current epoch: 0 ... training loss: 1.0273640339553591\n",
      "saving the model and checkpoint: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:38<00:00,  1.12it/s]\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running train loss: 0.6806072984624096: 100%|██████████| 380/380 [12:31<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing Current epoch: 1 ... training loss: 0.6832974458871622\n",
      "saving the model and checkpoint: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:28<00:00,  1.53it/s]\n",
      "  0%|          | 0/380 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running train loss: 0.6885164664053792: 100%|██████████| 380/380 [07:07<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finishing Current epoch: 2 ... training loss: 0.6930521480022526\n",
      "saving the model and checkpoint: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [00:21<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "detector.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../input/models/best_model_res152_epoch_2.pth'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy('best_model_epoch_2.pth', '../input/models/best_model_res152_epoch_2.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
